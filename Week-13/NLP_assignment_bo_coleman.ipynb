{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bo Coleman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Note: Yes, the notebook from the video is not provided, I leave it to you to make your own :) it's your final assignment for the semester. Enjoy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_sm\n",
    "# !python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'What is up my dudes, my name is Bo.  I live in NYC.  I like turtles.  I am originally from Arlington, Virginia'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "What is up my dudes, my name is Bo.  I live in NYC.  I like turtles.  I am originally from Arlington, Virginia"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_text = nlp(text)\n",
    "processed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 What is up my dudes, my name is Bo.\n",
      "1  I live in NYC.\n",
      "2  I like turtles.\n",
      "3  I am originally from Arlington, Virginia\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "for sentence in processed_text.sents:\n",
    "    print(n, sentence)\n",
    "    n+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words and Punctuation - Along with POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 What PRON what\n",
      "1 is AUX be\n",
      "2 up ADP up\n",
      "3 my PRON my\n",
      "4 dudes NOUN dude\n",
      "5 , PUNCT ,\n",
      "6 my PRON my\n",
      "7 name NOUN name\n",
      "8 is AUX be\n",
      "9 Bo PROPN Bo\n",
      "10 . PUNCT .\n",
      "11   SPACE  \n",
      "12 I PRON I\n",
      "13 live VERB live\n",
      "14 in ADP in\n",
      "15 NYC PROPN NYC\n",
      "16 . PUNCT .\n",
      "17   SPACE  \n",
      "18 I PRON I\n",
      "19 like VERB like\n",
      "20 turtles NOUN turtle\n",
      "21 . PUNCT .\n",
      "22   SPACE  \n",
      "23 I PRON I\n",
      "24 am AUX be\n",
      "25 originally ADV originally\n",
      "26 from ADP from\n",
      "27 Arlington PROPN Arlington\n",
      "28 , PUNCT ,\n",
      "29 Virginia PROPN Virginia\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "for sentence in processed_text.sents:\n",
    "    for token in sentence:\n",
    "        print(n, token, token.pos_, token.lemma_)\n",
    "        n+= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NYC LOC\n",
      "Arlington GPE\n",
      "Virginia GPE\n"
     ]
    }
   ],
   "source": [
    "for entity in processed_text.ents:\n",
    "    print(entity, entity.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noun Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What\n",
      "my dudes\n",
      "my name\n",
      "Bo\n",
      "I\n",
      "NYC\n",
      "I\n",
      "turtles\n",
      "I\n",
      "Arlington\n",
      "Virginia\n"
     ]
    }
   ],
   "source": [
    "for noun_chunk in processed_text.noun_chunks:\n",
    "    print(noun_chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Syntactic Depensy Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pr_tree(word, level):\n",
    "    if word.is_punct:\n",
    "        return\n",
    "    for child in word.lefts:\n",
    "        pr_tree(child, level+1)\n",
    "    print('\\t'* level + word.text + ' - ' + word.dep_)\n",
    "    for child in word.rights:\n",
    "        pr_tree(child, level+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tWhat - nsubj\n",
      "\tis - advcl\n",
      "\t\tup - prep\n",
      "\t\t\t\tmy - poss\n",
      "\t\t\tdudes - pobj\n",
      "\t\tmy - poss\n",
      "\tname - nsubj\n",
      "is - ROOT\n",
      "\tBo - attr\n",
      "-------------------------------------------\n",
      "\t  - dep\n",
      "\tI - nsubj\n",
      "live - ROOT\n",
      "\tin - prep\n",
      "\t\tNYC - pobj\n",
      "-------------------------------------------\n",
      "  - dep\n",
      "\t\tI - nsubj\n",
      "\tlike - prep\n",
      "\t\tturtles - pobj\n",
      "-------------------------------------------\n",
      "\t  - dep\n",
      "\tI - nsubj\n",
      "am - ROOT\n",
      "\toriginally - advmod\n",
      "\tfrom - prep\n",
      "\t\tArlington - pobj\n",
      "\t\t\tVirginia - appos\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for sentence in processed_text.sents:\n",
    "    pr_tree(sentence.root, 0)\n",
    "    print('-------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only print the first 2 to save space in html output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I [ 1.8733e-01  4.0595e-01 -5.1174e-01 -5.5482e-01  3.9716e-02  1.2887e-01\n",
      "  4.5137e-01 -5.9149e-01  1.5591e-01  1.5137e+00 -8.7020e-01  5.0672e-02\n",
      "  1.5211e-01 -1.9183e-01  1.1181e-01  1.2131e-01 -2.7212e-01  1.6203e+00\n",
      " -2.4884e-01  1.4060e-01  3.3099e-01 -1.8061e-02  1.5244e-01 -2.6943e-01\n",
      " -2.7833e-01 -5.2123e-02 -4.8149e-01 -5.1839e-01  8.6262e-02  3.0818e-02\n",
      " -2.1253e-01 -1.1378e-01 -2.2384e-01  1.8262e-01 -3.4541e-01  8.2611e-02\n",
      "  1.0024e-01 -7.9550e-02 -8.1721e-01  6.5621e-03  8.0134e-02 -3.9976e-01\n",
      " -6.3131e-02  3.2260e-01 -3.1625e-02  4.3056e-01 -2.7270e-01 -7.6020e-02\n",
      "  1.0293e-01 -8.8653e-02 -2.9087e-01 -4.7214e-02  4.6036e-02 -1.7788e-02\n",
      "  6.4990e-02  8.8451e-02 -3.1574e-01 -5.8522e-01  2.2295e-01 -5.2785e-02\n",
      " -5.5981e-01 -3.9580e-01 -7.9849e-02 -1.0933e-02 -4.1722e-02 -5.5576e-01\n",
      "  8.8707e-02  1.3710e-01 -2.9873e-03 -2.6256e-02  7.7330e-02  3.9199e-01\n",
      "  3.4507e-01 -8.0130e-02  3.3451e-01  2.7063e-01 -2.4544e-02  7.2576e-02\n",
      " -1.8120e-01  2.3693e-01  3.9977e-01  4.5012e-01  2.7179e-02  2.7400e-01\n",
      "  1.4791e-01 -5.8324e-03  9.5910e-01 -1.0129e+00  2.0699e-01  1.8237e-01\n",
      " -2.5234e-01 -2.6261e-01 -3.4799e-01 -2.4051e-02  4.4470e-01  5.9226e-02\n",
      "  4.5561e-01  1.9700e-01 -4.8327e-01  8.9523e-02 -2.2373e-01 -1.5654e-01\n",
      "  2.1578e-01  1.1673e-01  8.2006e-02 -8.0735e-01  2.3903e-01 -5.1304e-01\n",
      " -3.3888e-01 -3.1499e-01 -1.7272e-01 -6.7020e-01  2.7096e-01 -4.3241e-01\n",
      "  4.3103e-02  2.1233e-02  1.3350e-02 -6.3938e-02 -2.4957e-01 -2.4938e-01\n",
      "  3.4812e-01 -7.1321e-02  2.3375e-01 -9.5384e-02  5.2488e-01  6.8175e-01\n",
      " -1.0214e-01 -1.4914e-01 -7.5697e-02  1.7248e-01  2.5440e-01  1.5760e-01\n",
      " -5.9125e-01  2.4300e-01  6.3962e-01 -9.3280e-02 -2.7914e-01 -6.6262e-02\n",
      " -6.7170e-02 -4.0929e-01 -3.0300e+00  1.8250e-01  2.0113e-01  6.0628e-02\n",
      " -2.4769e-01  5.5324e-02 -4.9106e-01  3.1544e-01 -3.4231e-01 -6.3766e-01\n",
      " -3.6129e-01 -5.9029e-02  1.5510e-01  4.4577e-02  2.3572e-01 -1.7095e-01\n",
      " -2.2749e-01 -2.3184e-02  2.3868e-01  2.8170e-02  4.2965e-01 -1.2458e-01\n",
      " -3.6972e-02  2.0061e-01 -3.1405e-01 -8.5287e-02 -3.3496e-01 -9.7047e-02\n",
      " -1.4388e-01  1.1147e-01 -4.5232e-01 -2.4217e-01 -1.8245e-01 -6.7292e-01\n",
      "  2.1933e-02 -5.4816e-02 -4.6508e-01  4.7767e-01 -2.4752e-01 -1.5790e-01\n",
      "  1.1817e-01  5.6851e-02 -4.9151e-01  1.5496e-01  1.6425e-02  4.1650e-02\n",
      " -3.4990e-01 -1.5979e-01  3.9705e-01  2.2963e-01  2.4688e-01  1.9567e-02\n",
      " -2.8802e-01 -6.9983e-01  3.2744e-01  1.0833e-01  2.4945e-01 -7.8653e-01\n",
      " -6.1379e-02 -3.7359e-01 -1.1603e-01 -2.4950e-01  1.0161e-01  3.3994e-02\n",
      "  1.5650e-01  2.1344e-01 -1.1094e-01 -5.7687e-03  1.7869e-01 -1.0127e-01\n",
      " -1.6891e-02  3.0001e-01 -3.4116e-01 -3.2390e-02  4.2514e-02  1.1850e-01\n",
      " -1.8337e-01 -6.2865e-01 -2.8021e-01  4.2351e-01  1.1277e-01  1.2121e-03\n",
      "  1.5710e-01 -3.6321e-01 -4.9251e-01  1.1653e-01  2.4024e-01  1.7712e-01\n",
      "  6.8700e-02 -4.4137e-01 -2.9877e-01 -1.2071e-02  2.8325e-01  1.0668e-01\n",
      " -1.8859e-01 -4.1345e-01 -3.4090e-01  4.7236e-02 -3.8309e-01  4.3572e-01\n",
      "  2.4505e-01  2.7337e-01 -7.3038e-02  4.2514e-01 -3.2455e-02 -3.5211e-01\n",
      "  4.5691e-01  1.9433e-01 -1.5230e-01  4.2675e-01  2.8795e-01 -5.5969e-01\n",
      " -1.3031e-01  8.9844e-02  4.2605e-01 -1.9632e-01 -7.1989e-02 -8.0189e-02\n",
      " -3.0425e-01 -4.6190e-01  2.8178e-01 -9.9872e-02  3.5097e-01  1.6123e-01\n",
      " -3.6548e-02 -3.6739e-01 -1.9819e-02  3.2130e-01  1.7479e-01  2.5175e-01\n",
      " -7.6439e-03 -9.3786e-02 -3.7852e-01  4.3725e-01  2.1288e-01  2.5096e-01\n",
      " -1.9613e-01 -2.8865e-01 -5.6726e-03  4.2795e-01  2.0625e-01 -3.7701e-02\n",
      " -1.2200e-01 -7.9253e-02 -1.0290e-01  1.0558e-02  4.9880e-01  2.5382e-01\n",
      "  1.5526e-01  1.7951e-03  1.1633e-01  7.9300e-02 -3.9142e-01 -3.2483e-01\n",
      "  6.3451e-01 -1.8910e-01  5.4050e-02  1.6495e-01  1.8757e-01  5.3874e-01]\n",
      "think [-2.1788e-01  4.4128e-01 -4.3204e-01 -1.9803e-01 -2.7968e-03  2.8803e-01\n",
      "  8.0648e-02 -1.4643e-01  2.2300e-02  2.5941e+00 -2.5959e-01 -8.4183e-02\n",
      "  4.2613e-01  8.7662e-03 -3.3843e-01 -1.7814e-01 -4.2161e-01  3.3757e-01\n",
      " -3.9092e-01 -6.0522e-02  2.9517e-01  1.4590e-01  3.2846e-01 -5.8106e-02\n",
      " -1.9982e-01  1.1735e-01 -3.0825e-01 -2.2648e-01  4.7433e-01 -2.4415e-01\n",
      " -3.0177e-01  6.7390e-01  6.5974e-02  4.5855e-02 -3.6646e-02  9.9892e-02\n",
      "  1.3664e-01  1.6360e-01 -1.7109e-01 -2.4558e-01 -1.2538e-01  2.1834e-01\n",
      " -1.5026e-01  1.3336e-01  3.3971e-01  9.4071e-02 -5.1316e-01 -1.5819e-01\n",
      "  2.7468e-02 -3.4402e-02 -5.5910e-02  6.9633e-02 -1.2256e-02 -5.1804e-02\n",
      "  2.7225e-01 -1.8355e-01 -2.4559e-01 -3.1370e-01  1.5620e-01  6.3014e-02\n",
      " -3.2111e-01 -5.1905e-01 -1.1712e-01  3.6818e-01  4.0482e-02 -4.3880e-01\n",
      " -8.3865e-02  9.5061e-02  3.1995e-01  5.2088e-02  2.3889e-01 -1.6807e-03\n",
      "  3.1965e-01 -6.8731e-02  1.3477e-01  3.2888e-01  2.7228e-02 -1.9567e-01\n",
      " -1.0522e-01  4.3544e-01  1.8869e-02 -2.4586e-02  9.5041e-02 -1.0182e-01\n",
      "  2.2237e-01 -3.9997e-01  7.7279e-02 -8.7118e-01  3.3649e-01 -2.3464e-01\n",
      " -1.7064e-01 -1.6163e-01 -2.1596e-01  4.3201e-01  2.2237e-01  5.4215e-02\n",
      "  2.4430e-01  8.3594e-02  9.7403e-03  1.6315e-01 -1.7864e-01 -7.8538e-02\n",
      " -3.2577e-02 -1.3266e-01  4.1890e-01 -7.7905e-01 -2.1269e-01 -2.8179e-01\n",
      " -3.6263e-01  2.7969e-01  5.1118e-02 -4.3791e-01 -6.6222e-02 -2.9007e-01\n",
      "  9.8879e-02  6.8701e-02 -1.7953e-01 -2.4516e-01  6.2370e-02 -1.8344e-01\n",
      "  2.3848e-01 -3.8926e-01  1.4563e-02 -1.5011e-01  4.1463e-01  2.3519e-01\n",
      "  1.5768e-01 -4.2338e-01 -5.9981e-02 -8.5539e-02 -1.9645e-01  1.6238e-01\n",
      "  1.3915e-02  3.5895e-02  1.5087e-01 -1.2057e-01  2.5404e-03 -3.2900e-01\n",
      "  1.1253e-01 -1.1478e-01 -2.2095e+00  2.1364e-01  1.4400e-02  3.3077e-01\n",
      " -7.3231e-02 -3.3165e-01 -3.4116e-01  1.9457e-01 -2.5111e-01 -2.9986e-01\n",
      " -2.3206e-02 -4.6035e-02 -8.0109e-02  8.9126e-02  5.2734e-02 -9.5490e-02\n",
      "  2.0326e-02 -3.4696e-01 -1.3078e-01  4.9967e-02 -3.3681e-01  2.7430e-01\n",
      " -4.5855e-01 -5.0902e-02 -2.4579e-01 -9.9564e-02  2.7532e-01 -2.2780e-01\n",
      "  1.1668e-01 -9.2933e-02 -1.3185e-01 -2.0557e-01  6.7947e-02 -4.6485e-01\n",
      "  8.9792e-02 -1.6480e-02 -1.4847e-01  1.8379e-01  1.9782e-01 -8.6026e-02\n",
      "  9.0624e-02 -8.4892e-02 -3.7597e-01 -1.9855e-01 -1.2090e-02  1.2820e-02\n",
      "  7.7705e-02 -1.2059e-01  1.1353e-01  2.9137e-01  1.0847e-01  1.3505e-01\n",
      " -2.9519e-01 -3.0900e-01  1.1161e-01 -4.1132e-02 -7.6808e-02 -2.6873e-01\n",
      "  9.1791e-02  3.3636e-01  1.9916e-01 -1.8500e-01 -1.0462e-01 -3.0590e-01\n",
      "  1.5874e-01  9.2589e-02  2.1171e-02 -1.8780e-01  2.0077e-01  2.4509e-01\n",
      " -2.2700e-01 -2.1141e-01  1.9871e-02 -4.0445e-01  2.5579e-01  2.2388e-01\n",
      " -2.2641e-01 -7.4679e-02 -2.8030e-01 -1.1511e-01  1.2736e-01  1.9723e-01\n",
      "  3.1854e-02  3.5542e-02  1.6587e-01  1.0235e-01  2.4897e-01  1.0350e-01\n",
      " -4.2974e-02 -8.1860e-02 -4.3416e-01  1.0390e-01 -1.6777e-02  1.6120e-01\n",
      "  7.4300e-02 -9.9311e-02  1.8984e-01 -1.2747e-01 -6.0094e-02  4.9008e-01\n",
      "  1.0554e-01 -6.0390e-02  2.0226e-02  5.3835e-01 -7.1150e-02 -1.4458e-01\n",
      "  8.6395e-02 -5.1988e-02 -3.2138e-01  4.9567e-01  2.9081e-01 -2.8324e-01\n",
      " -1.6194e-01  1.0306e-02 -3.6499e-01 -4.5294e-02 -1.7151e-01 -5.6910e-02\n",
      "  1.6989e-01  1.8708e-01  4.0052e-01  2.4493e-01  1.2178e-01  3.4254e-01\n",
      "  1.5890e-01  8.0175e-02 -1.1652e-01  3.3864e-01  4.5713e-01  5.0961e-01\n",
      " -1.7444e-01 -1.1862e-02 -9.4227e-02 -4.2007e-01 -2.2938e-01  8.4131e-02\n",
      "  1.8915e-01 -3.5540e-01 -1.7737e-01  3.4414e-01 -1.9804e-01 -2.3456e-01\n",
      "  2.3658e-02 -4.2666e-02  7.3081e-02 -2.1149e-02  3.0316e-01 -2.9115e-01\n",
      "  8.3060e-02  3.4784e-02 -1.5084e-01  2.7544e-02 -2.7939e-01  3.8548e-02\n",
      "  2.2003e-01  1.8208e-01 -5.0746e-01 -1.6472e-01  3.2255e-01  3.0579e-01]\n",
      "green [-7.2368e-02  2.3320e-01  1.3726e-01 -1.5663e-01  2.4844e-01  3.4987e-01\n",
      " -2.4170e-01 -9.1426e-02 -5.3015e-01  1.3413e+00 -8.6785e-01 -1.3183e-01\n",
      " -5.9679e-01 -3.4415e-01 -1.6121e-01 -9.2512e-04  5.3267e-01  2.1329e+00\n",
      "  2.1933e-02 -5.1933e-01  3.6557e-01 -1.2978e-02 -2.7154e-01  4.8964e-03\n",
      " -1.1849e-01 -3.8338e-01 -4.8944e-01  4.9147e-01  1.3664e-01 -9.6163e-02\n",
      " -2.8429e-02  3.9630e-03  1.5542e-01 -2.9680e-01 -1.4895e-01 -5.5311e-02\n",
      "  3.0003e-01  1.6376e-01 -1.6941e-01 -1.0166e-01  5.2141e-01  8.5416e-02\n",
      "  1.6017e-02 -7.9741e-02  1.5934e-01  8.6290e-02 -2.1192e-01 -8.0312e-03\n",
      "  2.0699e-01 -2.0541e-01 -1.3612e-01  2.4044e-02 -1.7975e-02 -2.7537e-01\n",
      "  5.5046e-01 -7.4320e-01 -1.0718e-01  8.3590e-01  4.5894e-02 -8.3839e-03\n",
      " -3.7027e-01 -3.8694e-01  1.4741e-01 -6.2706e-02  5.5882e-01 -3.5788e-02\n",
      " -3.7742e-01 -2.5088e-01 -3.2712e-01 -2.1363e-02 -1.1778e-01  1.0936e-02\n",
      " -4.0838e-02  1.9662e-01 -2.0128e-01 -4.7566e-02  1.1487e-01 -9.0004e-02\n",
      "  1.0354e-01 -5.2373e-01 -1.1288e-01  2.3075e-01  4.3984e-01  5.4876e-01\n",
      "  1.9629e-01 -2.9718e-01  7.1773e-01  1.3269e+00  6.2276e-02 -8.8419e-02\n",
      "  3.5253e-01  6.1762e-01  6.2818e-01  2.1847e-02  1.1744e-01  1.4717e-01\n",
      "  2.4852e-02  3.1065e-01 -3.0706e-02 -4.8994e-01  1.9092e-01 -5.1000e-02\n",
      " -1.9395e-01 -4.9768e-01 -3.4417e-01 -8.2097e-01 -4.9253e-01  3.0066e-01\n",
      " -1.1905e-01  3.5405e-01 -5.9503e-01 -5.9864e-01 -9.2760e-02 -1.4563e-01\n",
      "  6.8754e-01  1.8893e-01 -4.6852e-02  1.0246e-01 -8.7789e-02 -3.2801e-01\n",
      "  3.1215e-01 -1.7373e-01 -3.4827e-01 -1.9547e-01  1.1008e-01  2.2747e-01\n",
      "  4.4502e-01  8.1171e-02 -3.8463e-03 -1.9223e-01 -1.6651e-01  3.9317e-02\n",
      "  2.3909e-01 -3.0472e-01 -2.9583e-01 -6.2451e-01  1.0243e-01 -2.3324e-01\n",
      "  5.0008e-01  8.9740e-02 -2.1251e+00  2.4246e-01  2.7600e-01  1.1749e-01\n",
      "  7.1881e-02  1.7860e-01 -4.4795e-03  1.5575e-01 -2.7073e-01 -8.8036e-02\n",
      " -1.1564e-02 -1.4186e-02  4.9359e-01  1.6096e-01 -4.4652e-01 -2.0159e-01\n",
      " -3.1921e-01  4.0095e-03 -3.9027e-01  2.6482e-01 -8.7063e-02  3.9982e-01\n",
      " -3.0174e-01  3.6335e-01  6.5750e-02 -4.8644e-01 -1.8118e-01 -7.6974e-01\n",
      "  1.7686e-01  3.7618e-01  1.1485e-01  9.7655e-03 -3.1654e-01  7.6573e-02\n",
      " -2.9506e-01 -2.2645e-01  6.8611e-01  6.6346e-02  2.2698e-01 -2.0357e-01\n",
      " -1.1136e-01 -3.9789e-02 -3.1132e-01 -3.9395e-01 -2.6340e-01  4.1417e-02\n",
      " -2.2766e-01 -1.5583e-01 -3.9518e-01 -1.7292e-01  3.4403e-01  4.0990e-01\n",
      " -9.3649e-02 -1.2536e-01  2.1836e-01  2.7454e-01  2.3929e-01  5.4202e-01\n",
      " -1.8898e-01  6.1104e-02 -9.9625e-02  6.9587e-02 -1.7275e-01  3.9217e-01\n",
      "  9.1343e-02  2.5958e-01  5.0131e-01  1.0328e-01  2.8023e-01 -4.2147e-01\n",
      " -2.3985e-01  5.0814e-01  4.0660e-01 -3.2745e-03  1.3557e-02  2.6442e-01\n",
      "  1.8914e-02 -1.9332e-02  2.0762e-01 -3.9842e-01 -5.6105e-01 -2.6695e-01\n",
      " -7.6739e-03  2.8867e-01  3.1247e-01 -4.4065e-03  3.4002e-01 -5.1330e-02\n",
      " -4.3934e-01  6.1596e-02  1.4591e-01  3.7920e-01  4.3088e-01  3.6122e-01\n",
      " -2.0847e-01  5.6458e-01 -5.6009e-02 -4.6236e-01  8.1828e-01  8.1877e-01\n",
      " -1.5978e-01 -3.0881e-01 -5.5235e-01  4.7371e-02 -3.8537e-02  3.7726e-01\n",
      "  6.0784e-02 -4.3161e-01 -3.3027e-01 -1.8559e-01  1.1674e-01 -1.3420e-01\n",
      " -2.0262e-01  8.2621e-02  3.2163e-01  2.5451e-01  1.3104e-01  5.2760e-01\n",
      " -4.7345e-03  1.9238e-01 -6.3701e-02  2.6855e-01  1.2537e-01  6.0333e-01\n",
      "  3.4068e-01 -3.6425e-01 -3.5315e-01 -4.3298e-01 -4.2086e-01  1.5704e-01\n",
      " -2.5552e-01  1.6895e-01  7.9552e-02 -3.1513e-01  8.5769e-02 -7.9049e-02\n",
      "  4.9882e-04  4.1551e-01  1.3062e-01  2.1869e-01  1.7056e-01 -2.3690e-01\n",
      " -3.9074e-01  5.9123e-02 -8.0229e-02  1.1957e-01  3.7294e-01  3.8980e-01\n",
      "  4.2767e-01 -1.1234e-01 -4.0517e-01  2.4357e-01  4.3730e-01 -4.6152e-01\n",
      " -3.5271e-01  3.3625e-01  6.9899e-02 -1.1155e-01  5.3293e-01  7.1268e-01]\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "for sent in proc_fruits.sents:\n",
    "    for token in sent:\n",
    "        if n < 3:\n",
    "            print(token, token.vector)\n",
    "        n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_fruits = nlp('''I think green apples are delicious. While pears have a strange texture to them. The bows they sit in are ugly''')\n",
    "apples, pears, bowls = proc_fruits.sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4225541055202484\n",
      "0.3517932891845703\n",
      "0.433989018201828\n"
     ]
    }
   ],
   "source": [
    "dude = processed_text.vocab['dudes']\n",
    "print(apples.similarity(dude))\n",
    "print(pears.similarity(dude))\n",
    "print(bowls.similarity(dude))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment\n",
    "Find your favorite news source and grab the article text.\n",
    "\n",
    "1. Show the most common words in the article.\n",
    "2. Show the most common words under a part of speech. (i.e. NOUN: {'Bob':12, 'Alice':4,})\n",
    "3. Find a subject/object relationship through the dependency parser in any sentence.\n",
    "4. Show the most common Entities and their types. \n",
    "5. Find Entites and their dependency (hint: entity.root.head)\n",
    "6. Find the most similar noun (chunks) in the article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am using the NYTimes as my news source.  The article can be found here:\n",
    "https://www.nytimes.com/2022/04/23/health/mental-health-crisis-teens.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx2txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "NYT_text = docx2txt.process('NYTimes_article.docx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "NYT = nlp(NYT_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Show the most common words in the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('M', 83),\n",
       " ('\\n\\n', 72),\n",
       " ('\\xa0', 40),\n",
       " ('said', 30),\n",
       " ('Linda', 27),\n",
       " ('school', 15),\n",
       " ('parents', 13),\n",
       " ('percent', 13),\n",
       " ('health', 11),\n",
       " ('Elaniv', 11)]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = [token.text for token in NYT if not token.is_stop and not token.is_punct]\n",
    "token_freqs = Counter(tokens)\n",
    "token_freqs.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further cleaning is required to remove the newline characters and other unwanted text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Show the most common words under a part of speech. (i.e. NOUN: {'Bob':12, 'Alice':4,})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>token</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>953</th>\n",
       "      <td>VERB</td>\n",
       "      <td>said</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>841</th>\n",
       "      <td>VERB</td>\n",
       "      <td>felt</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851</th>\n",
       "      <td>VERB</td>\n",
       "      <td>found</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934</th>\n",
       "      <td>VERB</td>\n",
       "      <td>recalled</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025</th>\n",
       "      <td>VERB</td>\n",
       "      <td>went</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>753</th>\n",
       "      <td>SYM</td>\n",
       "      <td>$</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>SPACE</td>\n",
       "      <td>\\n\\n</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752</th>\n",
       "      <td>SPACE</td>\n",
       "      <td></td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>SPACE</td>\n",
       "      <td>\\n\\n\\n\\n</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>SCONJ</td>\n",
       "      <td>like</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709</th>\n",
       "      <td>PROPN</td>\n",
       "      <td>M</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>707</th>\n",
       "      <td>PROPN</td>\n",
       "      <td>Linda</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>692</th>\n",
       "      <td>PROPN</td>\n",
       "      <td>Elaniv</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>690</th>\n",
       "      <td>PROPN</td>\n",
       "      <td>Dr.</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>738</th>\n",
       "      <td>PROPN</td>\n",
       "      <td>Tony</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>655</th>\n",
       "      <td>NUM</td>\n",
       "      <td>2019</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>634</th>\n",
       "      <td>NUM</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>639</th>\n",
       "      <td>NUM</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>642</th>\n",
       "      <td>NUM</td>\n",
       "      <td>1990</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>637</th>\n",
       "      <td>NUM</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>NOUN</td>\n",
       "      <td>school</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>NOUN</td>\n",
       "      <td>parents</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>NOUN</td>\n",
       "      <td>percent</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>NOUN</td>\n",
       "      <td>health</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>NOUN</td>\n",
       "      <td>adolescents</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>Hey</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>Oh</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>like</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>Huh</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>CCONJ</td>\n",
       "      <td>plus</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>AUX</td>\n",
       "      <td>having</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>AUX</td>\n",
       "      <td>felt</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>ADV</td>\n",
       "      <td>later</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>ADV</td>\n",
       "      <td>sharply</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>ADV</td>\n",
       "      <td>home</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>ADV</td>\n",
       "      <td>socially</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>ADV</td>\n",
       "      <td>ago</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>ADP</td>\n",
       "      <td>like</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>ADJ</td>\n",
       "      <td>mental</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>ADJ</td>\n",
       "      <td>high</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>ADJ</td>\n",
       "      <td>social</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ADJ</td>\n",
       "      <td>Black</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ADJ</td>\n",
       "      <td>adolescent</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       type        token  counts\n",
       "953    VERB         said      30\n",
       "841    VERB         felt       5\n",
       "851    VERB        found       5\n",
       "934    VERB     recalled       5\n",
       "1025   VERB         went       5\n",
       "753     SYM            $       1\n",
       "750   SPACE         \\n\\n      72\n",
       "752   SPACE                   40\n",
       "751   SPACE     \\n\\n\\n\\n       1\n",
       "749   SCONJ         like       2\n",
       "709   PROPN            M      82\n",
       "707   PROPN        Linda      27\n",
       "692   PROPN       Elaniv      11\n",
       "690   PROPN          Dr.       6\n",
       "738   PROPN         Tony       6\n",
       "655     NUM         2019       5\n",
       "634     NUM           10       3\n",
       "639     NUM           15       3\n",
       "642     NUM         1990       3\n",
       "637     NUM           13       2\n",
       "540    NOUN       school      15\n",
       "485    NOUN      parents      13\n",
       "492    NOUN      percent      13\n",
       "401    NOUN       health      11\n",
       "235    NOUN  adolescents      10\n",
       "217    INTJ          Hey       2\n",
       "219    INTJ           Oh       2\n",
       "220    INTJ         like       2\n",
       "218    INTJ          Huh       1\n",
       "216   CCONJ         plus       1\n",
       "215     AUX       having       2\n",
       "214     AUX         felt       1\n",
       "191     ADV        later       4\n",
       "206     ADV      sharply       4\n",
       "187     ADV         home       3\n",
       "208     ADV     socially       3\n",
       "172     ADV          ago       2\n",
       "167     ADP         like       4\n",
       "86      ADJ       mental       9\n",
       "64      ADJ         high       6\n",
       "135     ADJ       social       6\n",
       "1       ADJ        Black       4\n",
       "10      ADJ   adolescent       4"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_pos = [token.pos_ for token in NYT if not token.is_stop and not token.is_punct]\n",
    "NYT_df = pd.DataFrame({'token':tokens, 'type':token_pos})\n",
    "NYT_df = NYT_df.groupby(['type', 'token']).size().reset_index(name='counts')\n",
    "most_common = NYT_df.sort_values(['type', 'counts'], ascending=False).groupby('type').head(5)\n",
    "most_common"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Find a subject/object relationship through the dependency parser in any sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are three sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tMoments - npadvmod\n",
      "\tearlier - advmod\n",
      "\t\t\tthe - det\n",
      "\t\tgirl - poss\n",
      "\t\t\t’s - case\n",
      "\tmother - nsubj\n",
      "\t\tLinda - appos\n",
      "\thad - aux\n",
      "stolen - ROOT\n",
      "\t\ta - det\n",
      "\tlook - dobj\n",
      "\t\tat - prep\n",
      "\t\t\t\t\ther - poss\n",
      "\t\t\t\tdaughter - poss\n",
      "\t\t\t\t\t’s - case\n",
      "\t\t\tsmartphone - pobj\n",
      "-------------------------------------------\n",
      "\t\tThe - det\n",
      "\tteenager - nsubj\n",
      "\tincensed - advcl\n",
      "\t\tby - agent\n",
      "\t\t\t\tthe - det\n",
      "\t\t\tintrusion - pobj\n",
      "\thad - aux\n",
      "grabbed - ROOT\n",
      "\t\tthe - det\n",
      "\tphone - dobj\n",
      "\tand - cc\n",
      "\tfled - conj\n",
      "-------------------------------------------\n",
      "\t\tThe - det\n",
      "\tadolescent - nsubjpass\n",
      "\tis - aux\n",
      "\tbeing - auxpass\n",
      "identified - ROOT\n",
      "\tby - agent\n",
      "\t\t\tan - det\n",
      "\t\tinitial - pobj\n",
      "\t\t\tM - appos\n",
      "\t\t\t\tand - cc\n",
      "\t\t\t\t\tthe - det\n",
      "\t\t\t\tparents - conj\n",
      "\t\t\t\t\tby - prep\n",
      "\t\t\t\t\t\t\tfirst - amod\n",
      "\t\t\t\t\t\tname - pobj\n",
      "\t\t\tonly - advmod\n",
      "\t\tto - aux\n",
      "\tprotect - advcl\n",
      "\t\t\t\tthe - det\n",
      "\t\t\tfamily - poss\n",
      "\t\t\t\t’s - case\n",
      "\t\tprivacy - dobj\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "for sentence in NYT.sents:\n",
    "    if n > 1 and n < 5:\n",
    "        pr_tree(sentence.root, 0)\n",
    "        print('-------------------------------------------')\n",
    "    n += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Show the most common Entities and their types. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ent</th>\n",
       "      <th>ent_type</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Linda</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>Tony</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>first</td>\n",
       "      <td>ORDINAL</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>the United States</td>\n",
       "      <td>GPE</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>M</td>\n",
       "      <td>ORG</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Elaniv</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>one</td>\n",
       "      <td>CARDINAL</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Elaniv</td>\n",
       "      <td>ORG</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1990</td>\n",
       "      <td>DATE</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Elaniv Burnett</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ent  ent_type  counts\n",
       "63               Linda    PERSON      26\n",
       "87                Tony    PERSON       6\n",
       "100              first   ORDINAL       5\n",
       "117  the United States       GPE       4\n",
       "66                   M       ORG       3\n",
       "48              Elaniv    PERSON       3\n",
       "106                one  CARDINAL       3\n",
       "47              Elaniv       ORG       3\n",
       "11                1990      DATE       3\n",
       "49      Elaniv Burnett    PERSON       2"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ents = pd.DataFrame({'ent': [entity for entity in NYT.ents],\n",
    "                    'ent_type':[entity.label_ for entity in NYT.ents]})\n",
    "ents['ent'] = ents.apply(lambda x: str(x['ent']), axis=1)\n",
    "most_common_ents = ents.groupby(['ent', 'ent_type']).size().reset_index(name='counts')\n",
    "most_common_ents = most_common_ents.sort_values('counts', ascending=False).head(10)\n",
    "most_common_ents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Find Entites and their dependency (hint: entity.root.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the first 30 to save space in html doc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity | Dependency\n",
      "--------------------\n",
      "One evening  |  sprang\n",
      "last April  |  evening\n",
      "13-year-old  |  girl\n",
      "Minneapolis  |  in\n",
      "Linda  |  mother\n",
      "first  |  name\n",
      "Linda  |  alarmed\n",
      "Genocide Jack  |  were\n",
      "the preceding two years  |  In\n",
      "Linda  |  watched\n",
      "American  |  adolescence\n",
      "Three decades ago  |  came\n",
      "the United States  |  in\n",
      "2019  |  In\n",
      "13 percent  |  reported\n",
      "60 percent  |  increase\n",
      "2007  |   \n",
      "2000 to 2007  |  from\n",
      "nearly 60 percent  |  leaped\n",
      "2018  |  by\n",
      "the Centers for Disease Control and Prevention  |  to\n",
      "December  |  In\n",
      "U.S.  |  surgeon\n",
      "Candice Odgers  |  said\n",
      "the University of California  |  at\n",
      "Irvine  |  University\n",
      "Linda  |  realized\n",
      "Linda  |  talked\n",
      "Linda  |  jolted\n",
      "Linda  |  said\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "print('Entity | Dependency')\n",
    "print('--------------------')\n",
    "for entity in NYT.ents:\n",
    "    if n<30:\n",
    "        print(entity, ' | ', entity.root.head)\n",
    "    n += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Find the most similar noun (chunks) in the article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to runtime, I only compared the first 50 noun chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_comparison(chunk1, chunk2):\n",
    "    t1 = nlp(str(chunk1) +' . This to make similarity work.')\n",
    "    t2 = nlp(str(chunk2) +' . This to make similarity work.')\n",
    "    s1, s1p = t1.sents\n",
    "    s2, s2p = t2.sents\n",
    "    similarity = s1.similarity(s2)\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_chunks = [noun_chunk for noun_chunk in NYT.noun_chunks][:50]\n",
    "nc = pd.DataFrame({'noun_chunk':noun_chunks})\n",
    "nc['noun_chunk'] = nc.apply(lambda x: str(x['noun_chunk']), axis = 1)\n",
    "nc = nc.drop_duplicates()\n",
    "nc = nc.merge(nc, how = 'cross', suffixes=['_1', '_2'])\n",
    "nc = nc[nc['noun_chunk_1'] != nc['noun_chunk_2']]\n",
    "nc['similarity'] = nc.apply(lambda x: do_comparison(x['noun_chunk_1'], x['noun_chunk_2']), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get rid of consecutive rows where the same two noun chunks are being compared:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>noun_chunk_1</th>\n",
       "      <th>noun_chunk_2</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1130</th>\n",
       "      <td>intentional self-harm</td>\n",
       "      <td>self-harm</td>\n",
       "      <td>0.945473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>the backyard</td>\n",
       "      <td>the patio</td>\n",
       "      <td>0.885707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>Some</td>\n",
       "      <td>Others</td>\n",
       "      <td>0.874791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>the living room</td>\n",
       "      <td>the house</td>\n",
       "      <td>0.872049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>the girl’s mother</td>\n",
       "      <td>her daughter’s smartphone</td>\n",
       "      <td>0.871296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>The teenager</td>\n",
       "      <td>the girl’s mother</td>\n",
       "      <td>0.870744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1166</th>\n",
       "      <td>Others</td>\n",
       "      <td>who</td>\n",
       "      <td>0.864880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>642</th>\n",
       "      <td>The adolescent</td>\n",
       "      <td>The teenager</td>\n",
       "      <td>0.863191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768</th>\n",
       "      <td>the parents</td>\n",
       "      <td>The teenager</td>\n",
       "      <td>0.861646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1703</th>\n",
       "      <td>me</td>\n",
       "      <td>Some</td>\n",
       "      <td>0.852058</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               noun_chunk_1               noun_chunk_2  similarity\n",
       "1130  intentional self-harm                  self-harm    0.945473\n",
       "257            the backyard                  the patio    0.885707\n",
       "993                    Some                     Others    0.874791\n",
       "130         the living room                  the house    0.872049\n",
       "347       the girl’s mother  her daughter’s smartphone    0.871296\n",
       "512            The teenager          the girl’s mother    0.870744\n",
       "1166                 Others                        who    0.864880\n",
       "642          The adolescent               The teenager    0.863191\n",
       "768             the parents               The teenager    0.861646\n",
       "1703                     me                       Some    0.852058"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nc.sort_values('similarity', ascending=False).loc[[True, False]*int(nc.shape[0]/2)].head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
